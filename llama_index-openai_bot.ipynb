{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read files from the data directory and load them into a VectorStoreIndex   \n",
    "from llama_index.core import VectorStoreIndex,SimpleDirectoryReader \n",
    "documents = SimpleDirectoryReader(\"data\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='004022b8-3928-402c-ba3f-f174e81ec692', embedding=None, metadata={'page_label': '1', 'file_name': 'sample_data1.pdf', 'file_path': '/Users/anush/GenAI_projects/data/sample_data1.pdf', 'file_type': 'application/pdf', 'file_size': 968567, 'creation_date': '2024-09-01', 'last_modified_date': '2024-08-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\n \\n \\n \\n \\n \\nData  Science  \\nInterview  Questions  \\n(30 days of Interview  Preparation)', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='48f5e6db-f463-4ac1-97e6-595ee0f0db90', embedding=None, metadata={'page_label': '2', 'file_name': 'sample_data1.pdf', 'file_path': '/Users/anush/GenAI_projects/data/sample_data1.pdf', 'file_type': 'application/pdf', 'file_size': 968567, 'creation_date': '2024-09-01', 'last_modified_date': '2024-08-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='INEURON.AI  \\n \\n Page 2 \\n  \\nQ1.  What is the difference between AI,  Data  Science , ML, and DL ? \\n \\nAns 1 :  \\n         \\n  \\n \\nArtificial Intelligence : AI is purely math and scientific exercis e, but when it became computa tional , it \\nstarted to solve human problems formalized into a subset of computer science. A rtificial intelligence has \\nchanged the original computational statistics paradigm to the modern idea that machines could mimic \\nactual human capabilities, such as deci sion making and perfo rming more “human” tasks. Modern AI into \\ntwo categories  \\n1. General AI - Planning, decision making, identifying objects, recognizing sounds, social & \\nbusiness transactions  \\n2. Applied AI - driverless/ Autonomous car or machine smartly trade st ocks \\n \\nMachine Learning : Instead of engineers “teaching” or programming computers to have what they need \\nto carry out tasks, that perhaps computers could teach themselves – learn something without being \\nexplicitly programmed to do so. ML is a form of AI where  based on more data , and they can change \\nactions and response, which will make more effic ient, ad aptable and scalable. e .g., navigation apps and \\nrecommendation engine s. Classified into: - \\n1. Supervised  \\n2. Unsupervised  \\n3. Reinforcement learning  \\n \\nData Science : Data science ha s many tools, techniques, and algorithms called from these fields, plus \\nothers –to handle big data  \\nThe goal of data science, somewhat similar to machine learning, is to make accurate predictions and to \\nautomate and perform transactions in real -time, such as purchasing internet traffic or automatically \\ngenerating content.  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='47589f88-bb13-4f95-92e6-bbe4194802ec', embedding=None, metadata={'page_label': '3', 'file_name': 'sample_data1.pdf', 'file_path': '/Users/anush/GenAI_projects/data/sample_data1.pdf', 'file_type': 'application/pdf', 'file_size': 968567, 'creation_date': '2024-09-01', 'last_modified_date': '2024-08-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='INEURON.AI  \\n \\n Page 3 \\n Data science relies less on math and coding and more on data and building new systems to process the \\ndata. Relying on the fields of data integration, distributed architecture, aut omated machine learning, data \\nvisualization, data engineering, and automated data -driven decisions, data science can cover an entire \\nspectrum of data processing, not only the algorithms or statistics related to data.  \\n \\nDeep Learning : It is a technique for i mplementing ML.  \\nML provides the desired output from a given input , but DL reads the input and applies it to another data.  \\nIn ML , we can easily classify the flower  based upon the features . Suppose you want a machine to look at \\nan image and determine what  it represents to the human eye , whether a face, flower, landscape, truck, \\nbuilding, etc.  \\nMachine learning is not sufficient for this task because machine learning can only produce an output from \\na data set – whether according to a known algorithm or based on the inherent structure of the data. Y ou \\nmight be able to use machine learning to determine whether an image was of an “X” – a flower, say – and \\nit would learn and get more accurate. But that output is binary (yes/no) and is dependent on the \\nalgorithm, not the data. In the image recognition ca se, the outcome is not binary and not dependent on \\nthe algorithm.  \\nThe n eural network performs MICRO calculations with computational on many layers . Neural networks \\nalso support weighting data for ‘confidence . These results in a probabilistic system , vs. deterministic, and \\ncan handle tasks that we think of as requiring more ‘human -like’ judg ment.  \\n \\n \\nQ2. What is the difference  between Supervised learning, U nsupervised  learning and \\nReinforcement  learning ? \\n \\nAns 2 :  \\nMachine Learning  \\nMachine learning is the scient ific study of algorithms and statistical models that computer systems use to \\neffectively perform a specific task without using explicit instructions, relying on patterns and inference \\ninstead.  \\nBuilding a model by learning the patterns of historical data wi th some relationship between data to make \\na data -driven prediction.  \\n \\nTypes of Machine Learning  \\n• Supervised Learning  \\n• Unsupervised Learning  \\n• Reinforcement Learning  \\n \\nSupervised learning  \\nIn a supervised learning model, the algorithm learns on a labe led da taset, to generate reasonable \\npredictions for the response to new data.  (Forecasting outcome of new data)  \\n• Regression  \\n• Classification  \\n ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='46f54880-cd39-4530-9a17-8ba0c6902a26', embedding=None, metadata={'page_label': '4', 'file_name': 'sample_data1.pdf', 'file_path': '/Users/anush/GenAI_projects/data/sample_data1.pdf', 'file_type': 'application/pdf', 'file_size': 968567, 'creation_date': '2024-09-01', 'last_modified_date': '2024-08-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"INEURON.AI  \\n \\n Page 4 \\n  \\nUnsupervised learning  \\nAn unsupervised model, in contrast , provides unlabelled data that the algorithm tries to make sens e of by \\nextracting features, co -occur rence and underlying patterns on its own. We use unsupervised learning for  \\n• Clustering  \\n• Anomaly detection  \\n• Association  \\n• Autoencoders  \\nReinforcement Learning  \\nReinforcement learning is less supervised and depends on  the learning agent in determining the output \\nsolutions by arriving at different possible ways to achieve the best possible solution.  \\n \\nQ3. Describe the general architecture of Machine learning . \\n \\n          \\n  \\n \\n \\nBusiness understanding : Understand the give use cas e, and also , it's good to know more about the \\ndomain for which the use cases are buil t. \\n \\nData Acquisition and Understanding : Data gathering from different sources and understanding the \\ndata. Cleaning the data , handling the missing data if any , data wrangli ng, and EDA( Exp loratory data \\nanalysis) . \\n \\n \", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f12740ee-83cd-4c93-a63d-37b69d53e90c', embedding=None, metadata={'page_label': '5', 'file_name': 'sample_data1.pdf', 'file_path': '/Users/anush/GenAI_projects/data/sample_data1.pdf', 'file_type': 'application/pdf', 'file_size': 968567, 'creation_date': '2024-09-01', 'last_modified_date': '2024-08-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"INEURON.AI  \\n \\n Page 5 \\n Modeling:  Feature Engineering  - scaling the data , feature selection - not all feature s are important. We \\nuse the backward elimination method , correlation factors, PCA and domain knowledge to select the \\nfeatures.  \\nModel Training  based on trial and error method or by experience , we select the al gorithm and train with \\nthe selected features.  \\nModel evaluation  Accuracy of the model , confusion matrix and cross -validation.  \\nIf accuracy is not high , to achi eve higher accuracy , we tune  the model...either by changing the algo rithm \\nused or by feature selection or by g athering more data , etc. \\nDeployment  - Once the model has good accuracy , we deplo y the model either in the cloud or Rasberry \\npy or any other place. Once we deploy , we monitor the performa nce of the model.if its good...we go live \\nwith the model or re iterate the all process unti l our model performa nce is good.  \\nIt's not done yet!!!  \\nWhat if , after a few day s, our model performs bad ly because of new data . In that case , we do all the \\nprocess a gain by collecting new data and redeploy the model.  \\n \\nQ4. What is Linear Regression ? \\n \\nAns 4: \\nLinear Regression tends to establish a relationship between a depend ent variable(Y) and one or more \\nindepend ent variable(X) by finding the best fit of the straight line.  \\nThe equation for the Linear model is Y = mX+c, where m is the slope and c is the intercept  \\n \\n \\nIn the above diagram, the blue dots we see are the distribution of 'y' w.r.t 'x .' There is no straight line that \\nruns through all the data points. So, the objective here is to fit the best fit of a straight line that will try to \\nminimi ze the error between the expected and actual value . \\n \\n \", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='df76030c-1e6c-4eff-97bd-e248303eef80', embedding=None, metadata={'page_label': '6', 'file_name': 'sample_data1.pdf', 'file_path': '/Users/anush/GenAI_projects/data/sample_data1.pdf', 'file_type': 'application/pdf', 'file_size': 968567, 'creation_date': '2024-09-01', 'last_modified_date': '2024-08-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"INEURON.AI  \\n \\n Page 6 \\n  \\nQ5. OLS Stats Model (Ordinary Least Square)  \\n \\nAns 5: \\nOLS is a stats model, which will help us in identifying the more significan t features that can has an \\ninfluence on the ou tput. OLS model in python is  executed  as: \\nlm = smf.ols(formula = 'Sales ~ am+constant', data = data).fit() lm.conf_int() lm.summary()  \\nAnd we get the output as below,  \\n \\n \\nThe higher the t -value for the feature, the more significant the feature is to the output variable . And \\nalso, the p -value plays a rule in rejecting the Null hypothesis(Null hypothesis stating the features has zero \\nsignificance on the target variable.).  If the p -value is less than 0.05(95% confidence interval) for a \\nfeature, then we  can consider the feature to be significant.  \\n \\n \\nQ6. What is L1 Regularization (L1 = lasso)  ? \\n \\nAns 6: \\nThe main objective of creating a model(training data) is mak ing sure it fits the data properly and reduce \\nthe loss. Sometimes the model that is trained which will fit the data b ut it may fail and give a poor \\nperformance during analyzing of data (test data) . This leads to overfitting. Regularization came to \\novercome overfitting.  \\nLasso Regression ( Least Absolute Shrinkage and Selection Operator ) adds “ Absolute value of \\nmagnit ude” of coefficient , as penalty term to the loss function.  \", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a6e7de1f-edac-45d8-bb20-805abf7c0bed', embedding=None, metadata={'page_label': '7', 'file_name': 'sample_data1.pdf', 'file_path': '/Users/anush/GenAI_projects/data/sample_data1.pdf', 'file_type': 'application/pdf', 'file_size': 968567, 'creation_date': '2024-09-01', 'last_modified_date': '2024-08-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='INEURON.AI  \\n \\n Page 7 \\n Lasso shrinks the less important feature’s coefficient to zero ; thus, removing some feature altogether. So, \\nthis works well for feature selection in case we have a huge number of features.  \\n \\n \\n \\nMethods like Cross-validation, Stepwise Regression are there  to handle overfitting  and perform feature \\nselection work well with a small set of features . These techniques are  good when we are dealing with a \\nlarge set of features.  \\nAlong with sh rinking coefficients,  the lasso performs feature selection , as well. (Remember the \\n‘selection‘ in the lasso full -form?) Because some of the coefficients become exactly zero, which is \\nequivalent to the particular feature being excluded from the model.  \\n \\nQ7. L2 Re gularization(L2 = Ridge Regression)  \\n \\nAns 7: \\n \\n \\nOverfitting happens when the model learns signal as well as noise in the training data and wouldn’t \\nperform well on new/unseen data on which model wasn’t trained on.  \\nTo avoid overfitting your model on training data like  cross -validation sampling , reducing the number \\nof features, pruning , regularization , etc. \\nSo to  avoid overfitting , we perform  Regularization . \\n ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3a7152c1-9cfa-4c9a-b35a-42d4f488bab9', embedding=None, metadata={'page_label': '8', 'file_name': 'sample_data1.pdf', 'file_path': '/Users/anush/GenAI_projects/data/sample_data1.pdf', 'file_type': 'application/pdf', 'file_size': 968567, 'creation_date': '2024-09-01', 'last_modified_date': '2024-08-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='INEURON.AI  \\n \\n Page 8 \\n  \\n \\nThe Regression model that uses  L2 regularization is called Ridge Regression.  \\nThe f ormula for Ridge Regression :-  \\n \\n \\nRegula rization adds the penalty as model complexity increases. The r egularization parameter \\n(lambda) penalizes all the parameters except intercept so that the model generalizes the data and \\nwon’t overfit.  \\nRidge regression adds  “squared magnitude of the coefficient \" as penalty term to the loss \\nfunction. Here the box part in the above image represents the L2 regularization element/term.  \\n \\n \\nLambda is a hyperparameter.  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1dd7e4c3-61e3-485b-a2da-a36766e7ca70', embedding=None, metadata={'page_label': '9', 'file_name': 'sample_data1.pdf', 'file_path': '/Users/anush/GenAI_projects/data/sample_data1.pdf', 'file_type': 'application/pdf', 'file_size': 968567, 'creation_date': '2024-09-01', 'last_modified_date': '2024-08-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='INEURON.AI  \\n \\n Page 9 \\n If lambda is zero , then it is equivalent to OLS. But if lambda is very large , then it will add too much \\nweight , and it will lead to under -fitting . \\nRidge regularization  forces the weights to be small but does not make them zero  and does no t give \\nthe sparse solution . \\nRidge is not robust to outliers  as square terms blow  up the error differences of the outlie rs, and the \\nregularization term tries to fix it by penalizing the weights  \\nRidge regression performs better when all the input features influence the output , and all with  weights \\nare of roughly equal size . \\nL2 regularization can learn complex data patte rns. \\n \\nQ8. What is R square(where to use and where not) ? \\nAns 8. \\nR-squared  is a statistical measure of how close the data are to the fitted regression line. It is also \\nknown as the  coefficient of determination, or the coefficient of multiple determination for multiple \\nregre ssion.  \\nThe definition of R -squared is the  percentage of the response variable variation that is explained by a \\nlinear model.  \\nR-squared = Explained variation / Total variation  \\nR-squared is always between 0 and 100%.  \\n0% indicates that the model explains none  of the variability of the response data around its mean.  \\n100% indicates that the model explains all the variability of the response data around its mean.  \\nIn general, the higher the R -squared, the better the model fits your data.  \\n \\n \\n ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1b1b591a-4226-404b-a55c-46d129397ae1', embedding=None, metadata={'page_label': '10', 'file_name': 'sample_data1.pdf', 'file_path': '/Users/anush/GenAI_projects/data/sample_data1.pdf', 'file_type': 'application/pdf', 'file_size': 968567, 'creation_date': '2024-09-01', 'last_modified_date': '2024-08-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='INEURON.AI  \\n \\n Page 10 \\n There is a  problem w ith the R -Square. The problem arises when we ask this question to ourselves.** Is it \\ngood to help as many independent variables as possible?**  \\nThe answer is No  because we understood that each independent variable should have a meaningful \\nimpact. But, even**  if we add independent variables which are not meaningful**, will it improve R -Square \\nvalue?  \\nYes, this is the basic problem with R -Square. How many junk independent variables or important \\nindependent variable or impactful independent variable you add to y our model, the R -Squared value will \\nalways increase. It will never decrease with the addition of a newly independent variable , whether it could \\nbe an impactful, non -impactful , or bad variable, so we need another way to measure equivalent R -\\nSquare , which penalizes our model with any junk independent variable.   \\nSo, we calculate the  Adjusted R -Square  with a better adjustment in the formula of generic R -square.  \\n \\n \\nQ9. What is Mean Square  Error? \\nThe mean squared error tells you how close a regression line is to a set of points. It does this by \\ntaking the distances from the points to the regression line (these distances are the “errors”) and \\nsquaring them.  \\n \\nGiving an intuition  \\n \\n \\n \\nThe line equation is y=Mx+B . We want to find M (slope)  and B (y-intercept)  that minimizes the \\nsquared e rror. ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='06d7b41e-3d7b-4f97-b1d7-f1db98fe70c0', embedding=None, metadata={'page_label': '11', 'file_name': 'sample_data1.pdf', 'file_path': '/Users/anush/GenAI_projects/data/sample_data1.pdf', 'file_type': 'application/pdf', 'file_size': 968567, 'creation_date': '2024-09-01', 'last_modified_date': '2024-08-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='INEURON.AI  \\n \\n Page 11 \\n  \\n \\n \\n \\nQ10. Why S upport  Vector Regre ssion ? Difference between SVR and a simple regression \\nmodel?  \\n \\nAns 10: \\nIn simple linear regression , try to minimi ze the error rate. But in SVR , we try to  fit the er ror within \\na certain threshold . \\n \\nMain Concep ts:- \\n1. Boundary  \\n2. Kernel  \\n3. Support Vector  \\n4. Hyper Plane  \\n \\n \\n \\n \\nBlue line: Hyper Plane; Red Line: Boundary -Line \\n ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bacf150a-348a-406b-b2db-c02e356220ca', embedding=None, metadata={'page_label': '12', 'file_name': 'sample_data1.pdf', 'file_path': '/Users/anush/GenAI_projects/data/sample_data1.pdf', 'file_type': 'application/pdf', 'file_size': 968567, 'creation_date': '2024-09-01', 'last_modified_date': '2024-08-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='INEURON.AI  \\n \\n Page 12 \\n Our best fit line is the one w here th e hyperplane  has the maximum number of points.  \\nWe are trying to do here is trying to decide a decision boundary at ‘ e’ distance from the original \\nhyper plane such that data points closest to the hyper plane or the support vectors are within that \\nboundary line  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a5b2b7bd-6a3c-49c4-a0be-41f0ba8b729b', embedding=None, metadata={'page_label': '1', 'file_name': 'sample_data2.pdf', 'file_path': '/Users/anush/GenAI_projects/data/sample_data2.pdf', 'file_type': 'application/pdf', 'file_size': 1237363, 'creation_date': '2024-09-01', 'last_modified_date': '2024-08-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  1 | 16 \\n \\n \\n \\n \\n \\nDATA SCIENCE  \\nINTERVIEW  \\nPREPARATION  \\n(30 Days of Interview Preparation)  \\n \\n# Day28  \\n    ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='516e8611-af46-4f03-80fd-a934d355c4dd', embedding=None, metadata={'page_label': '2', 'file_name': 'sample_data2.pdf', 'file_path': '/Users/anush/GenAI_projects/data/sample_data2.pdf', 'file_type': 'application/pdf', 'file_size': 1237363, 'creation_date': '2024-09-01', 'last_modified_date': '2024-08-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  2 | 16 \\n \\nQ1. Explain StructEdit(Learning Structural Shape Variations) . \\nAnswer:  \\nThe shapes of the 3D objects exhibit remarkable diversity, both in their compositional structure in t erms \\nof parts, as well as in geometry  of the elemen ts themselves. Yet human  are remarkably skilled at \\nimagi ning meaningful shape variation  even from the isolated object instances. For example, having seen \\na new chair, we can easily imagine its natural  change s with the different height back, a wider seat, with \\nor without armrests, or wit h a di verse  base. In this article, we investigate how to learn such shape \\nvariations directly from the 3D data. Specifically, given the  shape collection, we are interested in two \\nsub-problems: first, for any give n shape, we want to discover  main modes of edi ts, which c an be inferred \\ndirectly from  shape collection; and second, given an example edit on on e shape, we want to transfer  edit \\nto another shape in the group , as a form of analogy -based edit transfer. This ability is useful in several \\nsettings, including the design of individual 3D models, the consistent modification of the 3D model \\nfamilies, and the fitting of CAD models to noisy and incomplete 3D scans.  \\n \\nAbove Fig: Edit generation and transfer with StructEdit.   \\nWe present the StructEdit, a method  that learns the distribution of  shape differences  between structured \\nobjects that can be used to generate an ample  variety of edits ( in a first row); and accurately transfer edits \\nbetween different purpose s and across different modalities ( on the second row). Edits can b e both \\ngeometric and topological.  \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6f745f97-a0fb-476b-acc7-efdb776d3750', embedding=None, metadata={'page_label': '3', 'file_name': 'sample_data2.pdf', 'file_path': '/Users/anush/GenAI_projects/data/sample_data2.pdf', 'file_type': 'application/pdf', 'file_size': 1237363, 'creation_date': '2024-09-01', 'last_modified_date': '2024-08-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  3 | 16 \\n \\nThere are many challenges in capturing  space of shape vari ations. First, individual shape  can have \\ndifferent representations as image , surface meshes , or point clouds; second, one needs th e unified setting \\nfor repre senting both continuous deformations as well as structural changes ; third, shape edits are not \\ndirectly expressed but are only implicitly contained in shape colle ctions; and finally, learning the space \\nof structural v ariations t hat is applicable to more than t he single shape amounts to learning mappings \\nbetween different shape edit distributions, since diffe rent shapes have various  types and numb ers of parts \\n(like tables with or without leg bars).  \\nIn much of the ex isting  literature on 3D machine learning (ML), 3D s hapes are mapped to poin ts in t he \\nrepresentation space whose coordinates encode latent f eatures of each shape. In such  representation, \\nshape edits are encoded as vectors in that same space – in other words , as differenc es between points \\nrepresenting shapes. Equivalently, we can think of form s as “anchored” vectors rooted at  origin, while \\nshape differences are “floating” vectors that can b e transported around in shape space. This type of vector \\nspace arithmetic i s commonly \\nused [wu2016learning , achlioptas2017learning , wang2018global , gao2018automatic , xia2015realtime , \\nVillegas_2018_CVPR ], for example, in performing analogies, where the vector that is the difference of \\npossible  point  A from point  B is added to point  C to produce an analogous point  D. The challenge with \\nthis view in our setting is that while Euclidean spaces are perfectly homogeneous and vectors can be \\ncomfortabl y transported and added to points anywhere, shape spaces are far or less so. While for \\ncontinuous variations , a vector space model has some plausibility, this is not so for structural variations: \\nthe “add arms” vector does not make sense for the point representing a chair that already has arms.  We \\ntake the different approach. We consider embedding  shape s differences or deltas  directly in their own \\nlatent space , separate from  general shape embedding space. Encoding and decoding such shape \\ndifferences is always done through a VAE ( variational autoencoder ), in the context of the given source \\nshape, i tself encoded through the  part hierarchy. This has the number of key advantages: (i)  allows \\ncompact encodings of shape deltas, since in general , we aim to describe local variation ; (ii) encourages  \\nnetwork to abstract commonalities in shape variations acros s shape s pace; and (iii)  adapts the edit  to the \\nprovided sou rce shape, suppressing the mode  that are semantically implausible.  \\nWe have extensively evaluated  the StructEdit  on publicly available  shape data sets. We introduce the  \\nnew synthetic dataset with g round truth shape edits to quantitatively evaluate our method and compa re \\nit against baseline alternative . We then p rovide evaluation results on  PartNet \\ndataset  [mo2019partnet ] and provide ablation studies. Finally, we demonstrate s that extension  of our \\nmethod allow s the handling  of both images and point cloud  as shape sources, can predict plausible edit \\nmodes from single shape examples, and can a lso transfer example shape edit  on one shape to  other shapes \\nin the collection . \\n ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a8d97733-bb0c-4e3d-b16d-fbf7430ca1c6', embedding=None, metadata={'page_label': '4', 'file_name': 'sample_data2.pdf', 'file_path': '/Users/anush/GenAI_projects/data/sample_data2.pdf', 'file_type': 'application/pdf', 'file_size': 1237363, 'creation_date': '2024-09-01', 'last_modified_date': '2024-08-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  4 | 16 \\n \\n \\nQ2. EmpGAN: Multi -reso lution Interactive Empathetic Dialogue \\nGeneration  \\nAnswer:  \\nAs a vital part of human intelligence, emot ional perceptivity is playing  elemental role in various social \\ncommunica tion scenarios, such as ., education  and healthcare systems . Recently, sensitive  conversation \\ngeneration has received an increasing amount of attention to address emotion factors in an end -to-end \\nframework . However, as  li2018syntactically   revealed , that conventional  emotional conversation system  \\naims to produce more emoti on-rich responses according to the specific user -input emot ion, which \\ninevitably leads to th e psychologic al inconsistency problem.  \\nStudies on social psychology suggest that empathy is the crucial step towards a more humanized human -\\nmachine conversa tion, which improves  emotional perceptivity in em otion -bonding social activities . To \\ndesign th e intelligent automatic dialogue system, it is essential  to make a chatbot empathetic within \\ndialogues . Therefore , in this paper, we focus on  a task of  empathetic dialogue generation , which  \\nautomatically tracks and understands the user’s emotion at each turn in multi -turn dialogue scenarios.  \\nDespite the achieved successes , obstacles to es tablishing the  empathetic conversational system are still \\nfar beyond current signs of progres s: \\n\\uf0b7 Mere ly conside ring the sentence -level emotion  while neglect ing more precise token -level \\nfeeling s may lead  to insufficient emotion perceptivity . It is challenging  to capture  nuances of \\nhuman emotion accurately without mode ling multi -granularity emotion factors in the dialogue \\ngeneration.  \\n\\uf0b7 Merely relying on the di alogue history but overlooking the potential of user feedback for the \\ngenerated responses further aggravates the deficiencies above , which causes undesirable \\nreaction s. \\nIn this paper, we propose the multi -resolution adversarial empathetic dialogue generation model, named \\nEmpGAN , to address the abov e challenges through generating more empathetic and appropriate  \\nresponse s. To capture  nuances of user feelings sufficiently, EmpGAN make responses by taking both \\ncoarse -grained sentence -level and fine -grained token -level emotions into account. The  response \\ngenerator in  EmpGAN dynamically understands sentiment s along wit h a conversat ion to perceive  a \\nuser’s emotion states in multi -turn conversations. Furthermore, an interactive adversarial learning \\nframework is augmented to take  user feedbac k into account thoughtfully, where two interactive \\ndiscriminators identify whether t he generated responses evoke  emotion perceptivity regarding b oth the \\ndialogue history and  user emotions.  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3e9f9c25-76f7-4207-9d88-b171ccdd77c8', embedding=None, metadata={'page_label': '5', 'file_name': 'sample_data2.pdf', 'file_path': '/Users/anush/GenAI_projects/data/sample_data2.pdf', 'file_type': 'application/pdf', 'file_size': 1237363, 'creation_date': '2024-09-01', 'last_modified_date': '2024-08-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  5 | 16 \\n \\nIn pa rticular, the EmpGAN contains the empathetic generator and two interactive inverse discriminators. \\nThe empathetic generator is composed of three components: ( i) A semantic understanding module based \\non Seq2Seq (sequence to sequence) neural networks that maintain the  multi -turn semantic context. ( ii) A \\nmulti -resolution emotion percep tion model captures the fine  and coarse -grained emotion factors of each \\ndialogue turn to build the emotional framework . (iii) An empathet ic response decoder combines  semantic \\nand emotional context to produce appropriat e responses in terms of both  meanin g and emotion. The two \\ninteractive inverse discriminator  additionally incor porate the user feedback and  corresponding emotional \\nfeedback as inverse supervised signal to induce the generator to produce a more empathetic response.  \\n \\nQ3. G-TAD: Sub -Graph Localization for Temporal Action Detection  \\nAnswer : \\nVideo understanding has gained much attention from both academia and industry over recent years, given  \\nthe rapid growth of videos published in online platforms. Tempora l action det ection is one of  exciting \\nbut challenging tasks in this  area. It involves detecting  start and the end frames of action instances, as \\nwell as predicting their class label . This is onerous , especially in long untrimmed videos.  \\n \\nVideo context is an important  cue to detect actions effectively . Here, we refer to mean as frames that are \\noutside the target action but carry valuable indicative information of it. Using video context to infer \\npotential actions is natural for human beings. Empirical evidence shows that human  can reliably predict \\nor guess the occu rrence of the specific  type of work  by only looking at short video snippets wh ere the \\naction does not happen . Therefore, incorporating context into tempora l action detection has become  \\nimportant strategy to boost detection acc uracy in the recent literature . Researchers have proposed various \\nways to take advantage  of the video context, such as extending temporal action bou ndaries by t he pre-\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a3bc6141-a522-43bb-8cb8-36c5c4bdafdd', embedding=None, metadata={'page_label': '6', 'file_name': 'sample_data2.pdf', 'file_path': '/Users/anush/GenAI_projects/data/sample_data2.pdf', 'file_type': 'application/pdf', 'file_size': 1237363, 'creation_date': '2024-09-01', 'last_modified_date': '2024-08-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  6 | 16 \\n \\ndefined ratio , using dilated convolution to encode meaning  into features , and aggregating definition  \\nfeature  implici tly by way of the  Gaussian curve . All these methods only utilize temporal context, which \\nfollows  or precedes  an action instance in its immediate secular  neighborhood. Howev er, real -world \\nvideos vary dramatically in temporal extent, action content, and even editing preferences. The use of \\nsuch temporal context s does not fully exploit  precious  merits of the video context, and it may also impair \\ndetection accuracy if not adequate ly de signed for underlying videos.  \\nSo, what properties characterize the desirable video context for accurate action detection? First, setting  \\nshould be semantically  or gra mma tically correlated to the target action other than merely temporally \\nlocated in its vicinity.  Imagine  a case where we manually stitch an action clip into some irrelevant frames ; \\nthe abrupt scene change surrounding the action would not benefit the action detection. On the other hand, \\nsnippets located at a distance from an opera tion but co ntaining similar semantic content might provide \\nindicative hints for detecting the action. Second, context should be content -adaptive rather than manually \\npre-defined. Considering the vast variation of videos, a framework  that helps to identify  different action \\ninstances could be changed  in lengths and locations based on the video content. Third, context should be \\nbased on multiple semantic levels, since using only one form/level of meaning  is unlikely to generalize \\nwell.  \\nIn this paper, we endow video context w ith all the above properties by casting action detection as a sub -\\ngraph localization problem based on a gra ph convolutional network (GCN) . We re present each video \\nsequence as the graph, each snippet as a node, each snippet -snippet correlation as an edge, and target \\nactions associated with context as sub -graphs, as  shown in Fig.  1. The meaning  of a snippet is considered \\nto be all snippets connected to it by an edge in a video graph. We define two types of edges — temporal \\ncorner s and semantic edges, each corresponding to temporal context and grammatical  context, \\nrespectively. Temporal edges exist between each pai r of neighboring snippets, whereas semantic edges \\nare dynamically learned from the video features at each GCN layer. Hence, the multi -level context of \\neach snippet is gradually aggregated into the features of the snippet throughout the entire GCN. ResNeXt \\ninspires the structure of each GCN block , so we name this G CN-based feature extractor GCNeXt.  \\nThe pipeline of our proposed Graph -Temporal Action Detection method, dubbed G -TAD , is analogous \\nto faster R -CNN  in objec t detection. There are two critical designs in G -TAD. First, GCNeXt, which \\ngenerates context -enriched features, corresponds to the backbone network, anal ogous to a series of \\nConvoluti onal Neural Network (CNN ) layers in faster R -CNN. Second, to mimic RoI(region of interest ) \\nalignment  in faster R -CNN, we design the sub-graph alignment (SGAlign) layer to generate a fixed -size \\nrepresentation for each sub-graph a nd embed all sub -graphs into  same Euclidean space. Finally, we apply \\na classifier on the features of each sub -graph to obtain detection results. We summarize our contributions \\nas follows.  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7ab03f71-bc0b-441f-a3c9-4b2c06bbfc84', embedding=None, metadata={'page_label': '7', 'file_name': 'sample_data2.pdf', 'file_path': '/Users/anush/GenAI_projects/data/sample_data2.pdf', 'file_type': 'application/pdf', 'file_size': 1237363, 'creation_date': '2024-09-01', 'last_modified_date': '2024-08-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  7 | 16 \\n \\n(1) We present a novel GCN -based video model to exploit video context for effective temporal action \\ndetection fully . Using this video GCN representation, we can adaptively incorporate multi -level semantic \\nmeaning  into the features of each snippet.  \\n(2) We propose G -TAD, a new sub -graph detection frame work , to localize actions in video graphs. G -\\nTAD includes two main modules: GCNeXt and SGAlign. GCNeXt performs graph convolutions on \\nvideo graphs, leveraging both temporal and semantic context. SGAlign re -arranges sub -graph features in \\nthe embedded space su itable for detection.  \\n(3) G-TAD achieves state -of-the-art(SOTA ) performance on two popular action detection benchmarks. \\nOn ActityNet -1.3, it achieves an average mAP of  34.09% . On THUMOS -14, it \\nreaches  40.16% mAP@0.5, beating all contemporary one -stage methods.  \\n \\nFig: Overview of G -TAD architecture.  The input of G -TAD is t he sequence of snippet features. We \\nfirst extract features using  b=3 GCNeXt blocks, which gradually aggregate both temporal and multi -level \\nsemantic context. Semantic context, encoded in semant ic edges, is dynamically learned from element s \\nat eac h GCNeXt layer. Then we feed extracted features into the SGAlign laye r, where sub -graphs defined \\nby the set of anchors are transformed to a fixed -size representation in the Euclidean space. Finally, th e \\nlocalization module scores and ranks the sub -graphs for detection.  \\n \\n \\n \\n \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='320cf81f-e9af-4a47-92e6-161bcdc4d46f', embedding=None, metadata={'page_label': '8', 'file_name': 'sample_data2.pdf', 'file_path': '/Users/anush/GenAI_projects/data/sample_data2.pdf', 'file_type': 'application/pdf', 'file_size': 1237363, 'creation_date': '2024-09-01', 'last_modified_date': '2024-08-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  8 | 16 \\n \\nQ4. What is F3Net? \\nAnswer:  \\n \\nF3Net is a combination of Fusion , Feedba ck, and Focus for Salient object detection (SOD) aims to \\nestimate the significant visual  regions of images or videos and  often serves as the pre -processing step f or \\nmany downstream vision tasks . Earlier SOD algorithms mainly rely on heuristic priors ( e.g., color, \\ntexture and contrast) to generate saliency maps. However, these hand -craft features can hardly capture \\nhigh-level semantic relations and context information . Thus they are not robust enough to complex \\nscenarios. Recently, convolutional neural networks (CNNs) have demonstrated its powerful feature \\nextraction capability i n visual feature representation.  Many CNNs -based models  have achieved \\nremarkable progress and pushed the performance of SOD to a new level. These models adopt the \\nencoder -decoder architecture, which is simple in structure and computationally efficie nt. The encoder \\nusually is made up of a pre -trained classification model (e.g. , ResNet  and VGG ), which can extract \\nmultiple features  of different semantic levels and resolutions. In the decoder, extracted features are \\ncombined to generate saliency  maps.  \\nHowever, there remain  two significant  challenges in accurate SOD. First, features of different levels have \\ndifferent distribution chara cteristics. High -level features have rich semantics but lack precis e location \\ninformation. Low -level features have rich details but full of background noises. To generate better \\nsaliency maps, multi -level features are combined. However, without delicate c ontrol of the information \\nflow in the model, some redundant features, including noises from low -level layers and coarse boundaries \\nfrom high -level layers , will pass in and possibly result in performance degradation. Second, most of the \\nexisting models use binar y cross -entropy that treats all pixels equally. Intuitively, different pixels deserve \\ndifferent weights,  e.g., pixels at the boundary are more discriminative and should be attached with more \\nimportance. Various boundary losses  have been proposed to enhance the boundary detection accuracy, \\nbut considering only the boundary pixels is not comprehensive e nough  since there are lots of pixels near \\nthe boundaries prone to wrong predictions. These pixels are also essential  and should be assigned with \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='37d80390-c983-4fed-a2f0-193c21e826d3', embedding=None, metadata={'page_label': '9', 'file_name': 'sample_data2.pdf', 'file_path': '/Users/anush/GenAI_projects/data/sample_data2.pdf', 'file_type': 'application/pdf', 'file_size': 1237363, 'creation_date': '2024-09-01', 'last_modified_date': '2024-08-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  9 | 16 \\n \\nlarger weights. In consequence, it is essential to design a mechanism to reduce the impact of \\ninconsistency be tween features of different levels and assign larger weights to those signific ant pixels.  \\nTo address the above challenges, we proposed a novel SOD framework, named F 3Net, which achieves \\nremarkable performance in producing high -quality saliency maps. First,  to mitigate the discrepancy \\nbetween features, we design a cross -feature module (CFM), which fuses element s of different levels by \\nelement -wise multiplication. Different from addition and concatenation, CFM takes a selective fusion \\nstrategy, where redundant information will be suppressed to avoid the contamination between features , \\nand important features will complement each other. Compared with traditional fusion methods, CFM can \\nremove background noises and sharpen boundaries, as shown in Fig.  1. Second, due to downsampling, \\nhigh-level fea tures may suffer from information loss and distortion, which can not be solved by CFM. \\nTherefore, we develop the cascaded feedback decoder (CFD) to refine these features iteratively. CFD \\ncontains multiple sub -decoders, each of which include s both bottom -up and top -down processes. For the \\nbottom -up method , multi -level features are aggregated by CFM gradually. For the top-down process, \\naggregated features are feedback into previous features to refine them. Third, we propose the pixel \\nposition -aware loss (PPA) to imp rove the commonly used binary cross -entropy loss , which treats all \\npixels equally. Pixels located at boundaries or elongated areas are more complicated  and discriminating. \\nPaying more attention to these hard pixels can further enhance model generaliz ation. PPA loss assigns \\ndifferent weights to different pixels, which extends binary cross -entropy. The weight of each pixel is \\ndetermined by its surrounding pixels. Hard pixels will get larger weights , and easy pixels will get smaller \\nones.  \\nTo demonstrate t he performance of F 3Net, we report experiment al results on five popular SOD datasets \\nand visualize some saliency maps. We conduct a series of ablation studies to evaluate the effect of each \\nmodule. Quantitative indicators and visual results show that F 3Net c an obtain  significantly better local \\ndetails and improved saliency maps. Codes ha ve been released. In short, our main contributions can be \\nsummarized as follows:  \\n\\uf0b7 We introduce the cross feature module to fuse features of different levels, which can extract the \\nshared parts between  features  and suppress each other’s background noises and complement each \\nother’s missing parts.  \\n\\uf0b7 We propose the cascaded feedback decoder for SOD, which can feedback features of both high \\nresolutions and high semantics to pr evious ones to correct and refine them for better saliency \\nmaps generation.  \\n\\uf0b7 We design pixel position -aware loss to assign different weights to different positions. It can better \\nmine the structure information contained in the features and help the network focus more on \\ndetail regions.  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bfd1c5f6-23f1-4a56-a01a-93c6ebd7aabd', embedding=None, metadata={'page_label': '10', 'file_name': 'sample_data2.pdf', 'file_path': '/Users/anush/GenAI_projects/data/sample_data2.pdf', 'file_type': 'application/pdf', 'file_size': 1237363, 'creation_date': '2024-09-01', 'last_modified_date': '2024-08-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  10 | 16 \\n \\n\\uf0b7 Experimental results demonstrate that the proposed model F 3Net achieves the state -of-the-art \\nperformance on five datasets in terms of six metrics, which proves the effectiveness and \\nsuperiority of the proposed method.  \\n \\nQ5.Natural Language Generation using Reinforcement Learning with \\nExternal Rewards  \\nAnswer:  \\nWe aim to develop models that are capable of generating language acro ss several genres of text , \\nconversational text s, and restaurant re views. After all, humans are ad ept at both. Extant NLG(natural \\nlanguage generation ) models work on either conversational text ( like movie dialogues) or longer text \\n(e.g., stories, reviews) but not both.  Also , while the state -of-the-art(SOTA ) in this field has advanc ed \\nquite rapidly, current model  is prone to generate language that is s hort, dull, off -context . More \\nimportantly,  a generated languag e may not adequately reflect  affective content of the input. Indeed, \\nhumans are already adept at t his task , as well. To address these re search challenges, we propose the \\nRNN -LSTM architecture that uses an encoder -decoder network. We also use reinforcement learning (RL) \\nthat incorporates internal and external rewards. Specifically, we use emotional appropriate ness as an \\ninternal reward for the NLG (Natural Language Generation) system – so that the emotional tone of \\ngenerated language is consistent with the emotional tone of p rior context fed as input to  model. We also \\neffectively incorporate usefulness scores as external rewards in our model. Our main contribution is the \\nuse of distantly labeled data in  architecture that generates coherent, affective content and we test the \\narchitecture across two different genres of text.  \\nWhat are the problem statement and their  intuition?  \\nOur aim is to take advantage of reinforcement learning (RL) and external rewards during the process of \\nlanguage generation. Complementary to this goal, we also aim  to generate language that has  same \\nemotional tone as the other  input. Emotions are recog nized as functional in decision -making by \\ninfluencing motivation and action selection.  However, the external fe edback and rewards are hard to \\ncome by for language generation; these would need to be provided  through crowdsourcing judgment  on \\ngenerated responses  during  generation proce ss, which makes  process time -consuming and impractical. \\nTo overcome th is problem, we look for distance labeling and use labels provided in  training set as a \\nproxy for human ju dgment  on generated responses. Particularly, we inc orporate usefulness scores in a \\nrestaurant review corpus as the proxy for external feedback.  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0de8b355-33bc-47dc-80eb-c838d66709e2', embedding=None, metadata={'page_label': '11', 'file_name': 'sample_data2.pdf', 'file_path': '/Users/anush/GenAI_projects/data/sample_data2.pdf', 'file_type': 'application/pdf', 'file_size': 1237363, 'creation_date': '2024-09-01', 'last_modified_date': '2024-08-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  11 | 16 \\n \\n \\nFig. 1:  Overall Architecture of the system showing internal and external rewards using reinforcement \\nlearning  \\nQ6. LaFIn: Generative Landmark Guided Face Inpainting  \\nAnswer:  \\nImage inpainting ( a.k.a.  image completion) refers to the process of reconstructing lost or deteriorated \\nregions of images, which can be applied to, as a fundamental component, various tasks suc h as image \\nrestoration and editing.  Undoubtedly, one expects the completed result to be realistic  so that the \\nreconstructed regions can be hardly perceived. Compared with natural scenes like oceans and lawns, \\nmanipulating faces, the focus of this work, is more challenging. Because the faces have much stronger \\ntopological structure and attribute consistency to preser ve. Figure  1 shows three such examples. Very \\noften, given the o bserved clues, human beings can easily infer what the lost parts possibly, although \\ninexactly, look like. As a consequence, a slight violation o f the topological structure and  the attribute \\nconsistency in the reconstructed face highly likely leads to a significant perceptual flaw. The following \\ndefines  the problem:  \\nDefinition:  \\nFace Inpainting. Given a face image , I with corrupted regions masked by  M. Let ¯¯¯¯¯¯M designate the \\ncomplement of  M and ∘ the Hadamard  product. The goal is to fill the target part with semantically \\nmeaningful and visually continuous information to the observed part. In other words, the completed \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a2bd916f-e178-4c9f-a3a1-3f768ae64526', embedding=None, metadata={'page_label': '12', 'file_name': 'sample_data2.pdf', 'file_path': '/Users/anush/GenAI_projects/data/sample_data2.pdf', 'file_type': 'application/pdf', 'file_size': 1237363, 'creation_date': '2024-09-01', 'last_modified_date': '2024-08-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  12 | 16 \\n \\nresult  ^I:=M∘^I+¯¯ ¯ ¯ ¯¯M∘I should preserve the topological structure among face components suc h as \\neyes, nose , and mouth, and the attribute consistency on like pose  gender, ethnicity , and expression.  \\n           \\nFigure 1:  Three face completion results by our method. From left to right: corrupted inputs, plus \\nlandmarks predicted from the inputs, and our final results, respectively.  \\n       \\nQ7. Image2StyleGAN++: How to Edit the Embedded Images?  \\nAnswer:  \\n \\n \\n(i)                             (ii)                              (iii)                                (iv)  \\nFrom above fig: (i) and ( ii): input images; ( iii): the “two -face” generated by naive ly copying the left half \\nfrom ( i) and the right half from (i i); (iv): the “two -face” created by our Image2StyleGAN++ framework.  \\nRecent GANs demonstrated that synthetic images c ould be generated with very high q uality. This \\nmotivates research into embedding algorithms that embed a given photograph into a GAN latent space. \\nSuch embedding algorithms can be used to analyze the limitations of GANs , do image inpainting, local \\nimage editing, global image transformations such as image m orphing and expression transfer, and few -\\nshot video generation.  \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e689ba60-84b5-4aef-a8bd-9ff6a01380c2', embedding=None, metadata={'page_label': '13', 'file_name': 'sample_data2.pdf', 'file_path': '/Users/anush/GenAI_projects/data/sample_data2.pdf', 'file_type': 'application/pdf', 'file_size': 1237363, 'creation_date': '2024-09-01', 'last_modified_date': '2024-08-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  13 | 16 \\n \\nIn this paper, we propose to extend a very recent embe dding algorithm, Image2 StyleGAN . In particular, \\nwe would like to improve this previous algorithm in three aspects. First, we noticed that the embedding \\nquality c ould be further improved by including No ise space optimization into  embedding framework. \\nThe key insight here is that stable Noise space optimizat ion can only be conducted if optimization is \\ndone sequentially with  W+ space and not jointly. Seco nd, we would like to improve  capabilities of the \\nembedding algorithm to increase the local control over the embedding. One way to improve local \\nauthority  is to include  mask  in embedding algorithm with undefined content. T he goal of the embedding \\nalgorithm should be to find a plausible embedding for everything outside the mask, while filling in \\nreasonable semantic content in the masked pixels.  \\nSimilarly, we would like to provide the option of approximate embeddings, where t he specified pixel \\ncolors are only a guide for the embedding. In this way, we aim to achieve high -quality embeddings that \\ncan be controlled by user scribbles. In the third technical part of the paper, we investigate the combination \\nof embedding algorithm a nd direct manipulations of the activation maps (called activation tensors in our \\narticle ). \\nQ8. oops!  Predicting Unintentional Action in Video  \\nAnswer:  \\nFrom just a glance at the video, we can  often tell whether a person’s action is intentional or not. For \\nexample, the Below figure shows a person attempting to jump off a raft, but unintentionally tripping into \\nthe sea. In a classic series of papers, developmental psychologist Amanda Woodward demonstrated that \\nchildren learn this ability to recognize the intentionality of actio n during their first year. However, \\npredicting the intention behind action has remained elusive for machine vision. Recent advances in action \\nrecognition have primari ly focused on predicting the physical motio ns and atomic opera tions in the video, \\nwhich captures the means of action but not the intent of action.  \\nWe believe a key limitation for perceiving visual intentionality has been the lack of realistic data with \\nnatural variation of intention. Although there are now extensive video datasets for action recognition, \\npeople are usually competent, which causes datasets to be biased towa rds successful outcomes. However, \\nthis bias for success makes discriminating and localizing visual intentionality  challenging for both \\nlearning and quantitative evaluation.  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='694284a3-a5a2-45ec-aeff-77263bd66dc3', embedding=None, metadata={'page_label': '14', 'file_name': 'sample_data2.pdf', 'file_path': '/Users/anush/GenAI_projects/data/sample_data2.pdf', 'file_type': 'application/pdf', 'file_size': 1237363, 'creation_date': '2024-09-01', 'last_modified_date': '2024-08-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\" \\n \\nP a g e  14 | 16 \\n \\n \\nFig: The oops! Dataset:  Each pair of frames shows an example of intentiona l and unintentional \\naction in our dataset. By crawling publicly available ‘‘fail’’ videos from the web, we can create a \\ndiverse and in -the-wild dataset of accident al action. For example, at the bottom -left cor ner shows \\na man failing to see  gate arm, and at the top -right shows two children playing  competitive game s \\nwhere it is inevitable ; one person will fail to accomplish their goal.  \\nWe introduce a new annotated video dataset that is abundant with unintentional action, which we have \\ncollected by crawling publicly available ‘‘fai l’’ videos from the web. From the above figure shows s ome \\nexamples, which cover in -the-wild situations for both intentional and unintentional action. Our video \\ndataset, which we will publicly release, is both large (over 50 hours of video) and diverse (covering \\nhundreds of scenes and activities). We annotate d videos with the temporal location at which the video \\ntransitions from intentional to unintentional action. We define three tasks on this dataset: classifying the \\nintentionality of action, localizing the change  from intentional to unintentional, and for ecasting  onset of \\nunintentional action shortly into the future.  \\nTo tackle these problems, we investigate several visual clues for learning with minimal labels to \\nrecognize intentionality. First, we propose a novel self -supervised task to learn to predi ct the speed of \\nthe video, which is incidental supervision available in all unlabeled video s for learning the action \\nrepresentation. Second, we e xplore the predictability of  temporal context as a clue to learn features, as \\nunintentional action often deviate s from expectation. Third, we study an order of events as a clue to \\nrecognize intentionality, since intentional action usually precedes unintentional action.  \\nExperiments and visualizations suggest that unlabeled video has intrinsic perceptual clues to rec ognize \\nintentionality. Our results show that, while each self -supervised task is useful, and learning to predict the \\nspeed of video helps the most. By ablating model and design choices, our analysis also suggests that \\nmodels do not rely solely on low -level mot ion clues to solve unintentional action prediction. Moreover, \\nalthough human 's consistency in our dataset is high, there is still a large gap in performance between our \\nmodels and human agreement, underscoring that analyzing h uman goals from videos remains t he \\n\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8bd6e5c1-b7e9-4535-ba21-15725f4b53ea', embedding=None, metadata={'page_label': '15', 'file_name': 'sample_data2.pdf', 'file_path': '/Users/anush/GenAI_projects/data/sample_data2.pdf', 'file_type': 'application/pdf', 'file_size': 1237363, 'creation_date': '2024-09-01', 'last_modified_date': '2024-08-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  15 | 16 \\n \\nfundamental challenge in computer vision (OpenCV). We hope this dataset of unintentional and \\nunconstrained action can provide the pragmatic benchmark of progress.  \\nQ9. FairyTED: A Fair Rating Predictor for TED Talk Data  \\nAnswer:  \\nIn recent times, artificial intelligence is bei ng used  for inconsequential decision making. Governments \\nmake use of it in the criminal justice system to predict \\nrecidivism  [brennan2009evaluating , tollenaar2013method ], which affects the decision about bail, \\nsentencing , and parole. Various firms are also using machine learning algorithms to examine and filter \\nresumes of job applicants  [nguyen2016hirability , chen2017automated , naim2016automated ], which is \\ncrucial for the growth of a company. Machine learning algorithms are also being used to evaluate \\nhuman’s socia l skills , such as presentation performance  [Chen2017a , Tanveer2015 ], essay grading.  \\nTo solve such decision -making problems, machine learning algorithms are trained on massive datasets \\nthat are usually collected in the wild. Due to difficulties in the manual  curation or adjustment over large \\ndataset s, the data likely  capture unwanted bias towards the underrepresented group based on race, gender , \\nor ethnicity. Such bias results in unfair decision -making systems, leading to unwanted and often \\ncatastrop hic consequences to human life and society. For example, the recognition rates of pedestrians \\nin autonomous vehicles are reported to be not equally accurate for all groups of \\npeople  [wilson2019predictive ]. Matthew et al.  [kay2015unequal ]showed that societa l bias gets reflected \\nin the machine learning algorithms through a biased dataset and causes representational harm for \\noccupations. Face recognition is not as useful  for people with different skin tones. Dark -skinned females \\nhave  43 times higher detection error than light -skinned males.  \\nIn this work, we propose a predictive framework that tackles the issue of designing a fair prediction \\nsystem from biased data. As an application scenario, we choose the problem of fair rating predicti on in \\nthe TED talks. TED talks cover a wide variety of topics and influence the audience by educating and \\ninspiring them. Also , it consists of speakers from a diverse community with imbalances in age, gender , \\nand ethnic attributes. The ratings are pr ovided by spontaneous visitors to the TED talk website. A \\nmachine learning algorithm trained solely from the audience ratings will have a possibility of the \\npredicted score  being biased by sensitive attributes of the speakers.  \\nIt is a challenging problem because numerous factors drive human behavior  and hence have huge \\nvariability. It is challenging  to know the way these factors interact with each other. Also , uncovering the \\ntrue interaction model may not be feasible and often expensive. Even though the sharing platforms such \\nas YouTube, Massive Open Online Courses (MOOC), or  ted.com  make it possible to collect a large \\namount of observational data, these platforms do not correct for bias and unfair ratings.  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ab2a0bab-51b7-46d5-b252-a7d80daf0da4', embedding=None, metadata={'page_label': '16', 'file_name': 'sample_data2.pdf', 'file_path': '/Users/anush/GenAI_projects/data/sample_data2.pdf', 'file_type': 'application/pdf', 'file_size': 1237363, 'creation_date': '2024-09-01', 'last_modified_date': '2024-08-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\nP a g e  16 | 16 \\n \\nIn th is work, we utilize  causal models  [pearl2009causal]  to define possible dependencies between \\nattributes of the data. We then address the problem  of not knowing  true interaction model by averaging \\noutputs of predictors across several possible causes. Furth er, using these causal models , we \\ngenerate  counterfactual samples  of sensitive attributes. These counterfactual samples are the key \\ncomponents in our fair prediction framework (adapted \\nfrom  kusner2017counterfactual  russell2017worlds) and help reducing b ias in ratings wrt sensitive \\nattributes. Finally, we introduce the  novel metric to quantify  degree of fairness employed by our \\nFairyTED pipeline. To  best o f our knowledge, FairyTED is  first fair prediction pipeline for  public \\nspeaking dataset s and can be applied to any dataset of  similar grounds. Apart from  theoretical \\ncontribution, our work also has practical implications in helping both the viewers and  organizers make \\ninformed and unbiased choices for the selection of talks an d speakers.  \\n \\n \\n \\n ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f4d5bf392724028b27026653e929061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3262e58815714b00812fc597602b7b42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = VectorStoreIndex.from_documents(documents,show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.core.indices.vector_store.base.VectorStoreIndex at 0x111510dd0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_enginre = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To have a custom query engine with to four results and a similarity cutoff of 80% \n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "retriever = VectorIndexRetriever(index=index,similarity_top_k=4)\n",
    "\n",
    "postprocessor = SimilarityPostprocessor(similarity_cutoff=0.80)\n",
    "\n",
    "query_engine = RetrieverQueryEngine(retriever=retriever,node_postprocessors=[postprocessor])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=query_enginre.query(\"What is supervised machinelearning?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supervised machine learning involves the algorithm learning from a labeled dataset to make predictions for new data. It focuses on forecasting outcomes based on historical data patterns and relationships. This type of machine learning includes techniques like regression and classification.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: Information about TF/IDF vectorization, Softmax non-\n",
      "linearity function, support vectors in SVM, different kernels in SVM,\n",
      "and a request to explain the Decision Tree algorithm in detail.\n",
      "______________________________________________________________________\n",
      "Source Node 1/2\n",
      "Node ID: 4f1d07aa-bb50-4164-9695-965dd9ee7440\n",
      "Similarity: 0.755260734530712\n",
      "Text: Follow Steve N ouri for more AI and Data science posts:\n",
      "https://lnkd.in/gZu463 X      Q26. Wha t is TF/IDF vectorization?\n",
      "TF–IDF is short for term frequency -inverse document frequency, is a\n",
      "numerical statistic that is intended to  reflect how important a word\n",
      "is to a document in a collection or corpus. It is often used as a wei\n",
      "ghting factor ...\n",
      "______________________________________________________________________\n",
      "Source Node 2/2\n",
      "Node ID: 3083a1dd-040f-4e4c-a689-0c66b55003d1\n",
      "Similarity: 0.743222114817079\n",
      "Text: Follow Steve N ouri for more AI and Data science posts:\n",
      "https://lnkd.in/gZu463 X in n-dimensional space with the value of each\n",
      "feature being the value of a particular coordinate. SVM uses\n",
      "hyperplanes to separate out different classes based on the provided\n",
      "kernel function.     Q46. What are the support vectors in SVM?     In\n",
      "the d iagram, we see...\n",
      "Information about TF/IDF vectorization, Softmax non-linearity function, support vectors in SVM, different kernels in SVM, and a request to explain the Decision Tree algorithm in detail.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.response.pprint_utils import pprint_response\n",
    "\n",
    "pprint_response(response,show_source=True)    \n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
